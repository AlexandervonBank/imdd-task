{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a SNN Demapper with Receptive-field Encoding\n",
    "\n",
    "More details can be found in [E. Arnold et al., “Spiking neural network nonlinear\n",
    "demapping on neuromorphic hardware for IM/DD optical communication”](https://ieeexplore.ieee.org/abstract/document/10059327/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import norse.torch as norse\n",
    "\n",
    "from IMDD import PAM4IMDD, IMDDModel, IMDDParams, helpers\n",
    "from IMDD.snn.encoding import ReceptiveFieldEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive-field Encoding\n",
    "\n",
    "One question when training a spiking demapper is how to best translate a chunk of real-valued data into a spiking representation in an efficient way.\n",
    "The receptive-field encoding translates each samples $y_k$ in the chunk to a set of $K$ spiking neurons.\n",
    "Each neuron has a `reference_point` assigned and its spike time is determined by the distance of $y_k$ to the given reference value.\n",
    "This results in a spatio-temporal encoding with $K$ neurons per sample $k$.\n",
    "\n",
    "We first create an IMDD link to generate data in order to visualize the input encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = IMDDParams(\n",
    "    N=10000,\n",
    "    n_taps=7,\n",
    "    alphabet=torch.tensor([-3., -1., 1., 3.]),\n",
    "    oversampling_factor=3,\n",
    "    baudrate=112000000000,\n",
    "    wavelength=0.000001270,\n",
    "    dispersion_parameter=-0.000005,\n",
    "    fiber_length=4000,\n",
    "    noise_power_gain_db=20.,\n",
    "    roll_off=0.2,\n",
    "    bias=2.2)\n",
    "link = IMDDModel(params)\n",
    "\n",
    "## Generate some data\n",
    "# Samples to send\n",
    "samples = link.source()\n",
    "print(\"Send symbols:\\n\", samples, samples.shape)\n",
    "\n",
    "# Received samples\n",
    "chunks = link(samples)\n",
    "print(\"Received data (chunked):\\n\", chunks, chunks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an instance of the `ReceptiveFieldEncoder`. The encoding is defined by the `references` which we choose to be $10$ values equdistantly distributed in $[0, 7]$. The distance $y_k - \\Chi_i$ is scaled by `scaling`. We neglect spikes which are later than `cutoff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "references = torch.linspace(0, 7, K)\n",
    "print(\"References: \", references)\n",
    "\n",
    "# Time resolution of encoding and SNN\n",
    "dt = 5e-4\n",
    "time_length = 0.03  # s\n",
    "cutoff = 0.015  # s\n",
    "\n",
    "# The encoer\n",
    "encoder = ReceptiveFieldEncoder(\n",
    "    scaling=0.008,\n",
    "    offset=0.0,\n",
    "    time_length=time_length,\n",
    "    dt=dt,\n",
    "    references=references,\n",
    "    cutoff=cutoff,\n",
    "    inverse=False)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode the 10 first chunks into binary spike tensors. The first dimension in the resulting data is the time axis, which is 60, corresponding to 30 ms. This becomes clearer in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = encoder(chunks[:10])  # first ten samples\n",
    "print(\"Spikes:\\n\", spikes, spikes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time-dense spikes into event-based spikes\n",
    "events = torch.nonzero(spikes[:, 0])\n",
    "time = np.linspace(0, time_length, int(time_length / dt))\n",
    "\n",
    "fig, axes = plt.subplots(1)\n",
    "axes.set_xlim(0, time_length)\n",
    "axes.set_ylim(0, params.n_taps * K)\n",
    "axes.set_xlabel(\"$t$ [ms]\")\n",
    "axes.set_ylabel(\"input neuron $i$\")\n",
    "axes.scatter(events[:, 0] * dt, events[:, 1], s=5, color=\"black\")\n",
    "axes.vlines(cutoff, 0, params.n_taps * K, color=\"blue\", ls=\":\")\n",
    "for i in range(7):\n",
    "    axes.hlines(10 * (i + 1), 0, 0.015, color=\"grey\", lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNDemapper(torch.nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_in: int,\n",
    "                 n_hidden: int,\n",
    "                 n_out: int,\n",
    "                 encoder: torch.nn.Module,\n",
    "                 lif_params: norse.LIFParameters,\n",
    "                 li_params: norse.LIParameters,\n",
    "                 dt: float,\n",
    "                 device):\n",
    "        \"\"\" \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.dt = dt\n",
    "        # Regularization\n",
    "        self.reg_bursts = 0.0005\n",
    "        self.reg_weight_1 = 0.0001\n",
    "        self.reg_weight_2 = 0.0001\n",
    "        self.reg_readout = 0.0\n",
    "        self.target_rate = 0.5\n",
    "\n",
    "        # Encoding symbols to spikes\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # SNN\n",
    "        self.linear_1 = torch.nn.Linear(n_in, n_hidden, device=device, bias=None)\n",
    "        self.lif = norse.LIFCell(lif_params)\n",
    "        self.linear_2 = torch.nn.Linear(n_hidden, n_out, device=device, bias=None)\n",
    "        self.li = norse.LICell(li_params)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \"\"\"\n",
    "        zi = self.encoder(input).float()\n",
    "\n",
    "        T = zi.shape[0]\n",
    "        s_lif, s_li = None, None\n",
    "        zs, ys, s_lifs, s_lis = [], [], [], []\n",
    "        for ts in range(T):\n",
    "            g1 = self.linear_1(zi[ts])\n",
    "            z, s_lif = self.lif(g1, s_lif)\n",
    "            g2 = self.linear_2(z)\n",
    "            y, s_li = self.li(g2, s_li)\n",
    "\n",
    "            zs.append(z)\n",
    "            ys.append(y)\n",
    "            s_lifs.append(s_lif)\n",
    "            s_lis.append(s_li)\n",
    "\n",
    "        self.spikes = torch.stack(zs)\n",
    "        self.traces = torch.stack(ys)\n",
    "        self.v_lif = torch.stack([s.v for s in s_lifs])\n",
    "\n",
    "        self.score = torch.amax(self.traces, 0)\n",
    "\n",
    "        return self.score\n",
    "\n",
    "    def regularize(self) -> torch.Tensor:\n",
    "        \"\"\" Regularization terms for demapper \"\"\"\n",
    "        reg = torch.tensor(0.).to(self.device)\n",
    "        # Regularize linear weights\n",
    "        reg += self.reg_weight_1 * torch.mean(self.linear_1.weight ** 2)\n",
    "        reg += self.reg_weight_2 * torch.mean(self.linear_2.weight ** 2)\n",
    "        # Regularize firing rates\n",
    "        reg += self.reg_bursts * (\n",
    "            (self.target_rate - self.spikes.sum(0)).mean(0) ** 2).mean()\n",
    "        # Regularize readout traces\n",
    "        reg += self.reg_readout * torch.mean(torch.max(self.traces, 0)[0] ** 2)\n",
    "        return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stats(loss: torch.Tensor, pred: torch.Tensor, data: torch.Tensor):\n",
    "    ber = helpers.bit_error_rate(data, pred, False)\n",
    "    acc = helpers.accuracy(data, pred, False)\n",
    "    count = torch.count_nonzero(torch.argmax(pred, 1) != data)\n",
    "    return ber, acc, count\n",
    "\n",
    "\n",
    "def train(dataloader, optimizer, scheduler, loss_fn, demapper, device):\n",
    "    loss, acc, ber = [], [], []\n",
    "\n",
    "    for i, (data, target) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        pred_b = demapper(data)\n",
    "        loss_b = loss_fn(pred_b, target)\n",
    "        # regularization\n",
    "        loss_b += demapper.regularize()\n",
    "\n",
    "        # Optimize\n",
    "        loss_b.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get stats\n",
    "        ber_b, acc_b, _ = stats(loss_b, pred_b, target)\n",
    "\n",
    "        # Accumualte\n",
    "        loss.append(loss_b.detach())\n",
    "        acc.append(acc_b)\n",
    "        ber.append(ber_b)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return (torch.stack(loss).reshape(-1).mean(),\n",
    "            np.stack(acc).reshape(-1).mean(),\n",
    "            np.stack(ber).reshape(-1).mean())\n",
    "\n",
    "\n",
    "def test(dataloader, demapper, loss_fn, device, min_false_symbols, max_test_epochs):\n",
    "    loss, acc, ber, n_false = [], [], [], 0\n",
    "\n",
    "    for epoch in range(max_test_epochs):\n",
    "        for i, (data, target) in enumerate(dataloader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pred_b = demapper(data)\n",
    "            loss_b = loss_fn(pred_b, target)\n",
    "            loss_b += demapper.regularize()\n",
    "\n",
    "            ber_b, acc_b, count = stats(loss_b, pred_b, target)\n",
    "\n",
    "            loss.append(loss_b.detach())\n",
    "            acc.append(acc_b)\n",
    "            ber.append(ber_b)\n",
    "\n",
    "            n_false += count\n",
    "\n",
    "        if n_false >= min_false_symbols:\n",
    "            break\n",
    "\n",
    "    return (torch.stack(loss).reshape(-1).mean(),\n",
    "            np.stack(acc).reshape(-1).mean(),\n",
    "            np.stack(ber).reshape(-1).mean(), n_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "# Training parameters\n",
    "batch_size_train = 100 \n",
    "batch_size_val = 10000 \n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "min_false_symbols = 1000\n",
    "max_test_epochs = 100\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "lif_demapper = SNNDemapper(\n",
    "    n_in=70,  # n_taps * n_reference_points\n",
    "    n_hidden=40,\n",
    "    n_out=4,  # len(alphabet)\n",
    "    encoder=encoder,\n",
    "    lif_params=norse.LIFParameters(\n",
    "        tau_mem_inv=1/6e-3,\n",
    "        tau_syn_inv=1/6e-3,\n",
    "        v_leak=0.,\n",
    "        v_reset=0.,\n",
    "        v_th=1.),\n",
    "    li_params=norse.LIParameters(\n",
    "        tau_mem_inv=torch.tensor(1/6e-3).to(device),\n",
    "        tau_syn_inv=torch.tensor(1/6e-3).to(device),\n",
    "        v_leak=torch.tensor(0.)),\n",
    "    dt=dt,\n",
    "    device=device)\n",
    "\n",
    "# Dataset\n",
    "dataset = PAM4IMDD(params)\n",
    "\n",
    "# Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size_train, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size_val, shuffle=True)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# The SNRs we train the demapper for\n",
    "snrs = torch.flip(torch.arange(15., 24., 1.), dims=(0,))\n",
    "snrs[0] = 30. # We train the first demapper with only a little noise\n",
    "\n",
    "# Validation data\n",
    "val_datas = torch.zeros((snrs.shape[0], epochs // 10, 4))\n",
    "\n",
    "model_dir = Path(\"./models\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for i, snr in enumerate(snrs):\n",
    "    print(f\"SNR: {snr.item()}\")\n",
    "    # update SNR in Dataset\n",
    "    dataset.simulator.params.noise_power_gain_db = snr.item()\n",
    "\n",
    "    # New scheduler\n",
    "    optimizer = torch.optim.Adam(lif_demapper.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "    # train for SNR\n",
    "    # pbar = tqdm(total=epochs, unit=\"epoch\")\n",
    "    val_data = torch.zeros((epochs // 10, 4))\n",
    "    best_val_ber = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc, train_ber = train(\n",
    "            train_loader, optimizer, scheduler, loss_fn, lif_demapper, device)\n",
    "        print(f\"Epoch={epoch}, loss={train_loss:.4f}, ber={train_ber:.7f}, \"\n",
    "              + f\"acc={train_acc:.4f}\")\n",
    "        if (epoch + 1) % 10 == 0 and epoch > 0:\n",
    "            val_loss, val_acc, val_ber, n_false = test(\n",
    "                val_loader, lif_demapper, loss_fn, device, min_false_symbols,\n",
    "                max_test_epochs)\n",
    "            val_datas[i, epoch // 10, 0] = val_loss\n",
    "            val_datas[i, epoch // 10, 1] = val_ber\n",
    "            val_datas[i, epoch // 10, 2] = val_acc\n",
    "            val_datas[i, epoch // 10, 3] = n_false\n",
    "\n",
    "            # Save best Demapper\n",
    "            if val_ber < best_val_ber:\n",
    "                torch.save(\n",
    "                    lif_demapper.state_dict(), f\"./models/snr_{int(snr)}.pt\")\n",
    "                best_val_ber = val_ber\n",
    "\n",
    "            print(f\"Epoch={epoch}, val_loss={val_loss:.4f}, \"\n",
    "                  + f\"val_ber={val_ber:.7f}, val_acc={val_acc:.4f}, \"\n",
    "                  + f\"n_false={n_false}\")\n",
    "\n",
    "    # Save data\n",
    "    np.save(\"snrs.npy\", val_datas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test demapper on independet data\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "min_false_symbols = 2000\n",
    "\n",
    "# Dataset and loader\n",
    "dataset = PAM4IMDD(params)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size_val, shuffle=False)\n",
    "\n",
    "bers = np.zeros((snrs.shape[0] - 1, 3))\n",
    "\n",
    "for s, snr in enumerate(snrs[1:]):\n",
    "    # Set SNR in dataset\n",
    "    dataset.simulator.params.noise_power_gain_db = snr.item()\n",
    "\n",
    "    # Load best model for current SNR\n",
    "    state_dict = torch.load(f\"./models/snr_{int(snr)}.pt\")\n",
    "    lif_demapper.load_state_dict(state_dict)\n",
    "\n",
    "    loss, acc, ber, n_false = [], [], [], 0\n",
    "\n",
    "    ber = []\n",
    "    while True:\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pred_b = lif_demapper(data)\n",
    "\n",
    "            ber_b = helpers.bit_error_rate(target, pred_b, False)\n",
    "            ber.append(ber_b)\n",
    "\n",
    "            n_false += torch.count_nonzero(torch.argmax(pred_b, 1) != target)\n",
    "\n",
    "        if n_false >= min_false_symbols:\n",
    "            break\n",
    "\n",
    "    bers[s, 0] = snr\n",
    "    bers[s, 1] = np.stack(ber).reshape(-1).mean()\n",
    "    bers[s, 2] = n_false\n",
    "\n",
    "    print(f\"Tested Demapper for {snr}. BER = {bers[s, 1]}, n_false = {n_false}\")\n",
    "\n",
    "np.save(\"test_bers.npy\", bers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first sample in last batch\n",
    "events = torch.nonzero(lif_demapper.spikes.detach().cpu()[:, 0]).numpy()\n",
    "\n",
    "_, axs = plt.subplots(nrows=3)\n",
    "for i in range(3):\n",
    "    axs[i].set_xlim(0, 60)\n",
    "axs[0].scatter(events[:, 0], spikes[:, 1])\n",
    "axs[1].plot(lif_demapper.v_lif.detach().cpu().numpy()[:, 0])\n",
    "axs[2].plot(lif_demapper.traces.detach().cpu().numpy()[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot BER-SNR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"./test_bers.npy\")\n",
    "\n",
    "import pickle\n",
    "with open(\"../huawei/results/data/imdd/ecoc_jlt/ber_snr_references/1tap_linear.pkl\", \"rb\") as file:\n",
    "    data_linear_1tap = pickle.load(file)\n",
    "\n",
    "with open(\"../huawei/results/data/imdd/ecoc_jlt/ber_snr_references/7tap_linear.pkl\", \"rb\") as file:\n",
    "    data_linear_7tap = np.array(pickle.load(file))\n",
    "\n",
    "with open(\"../huawei/results/data/imdd/ecoc_jlt/hw/snr_sweep_additive/n_hidden_40/n_taps_7/test_bers.pkl\", \"rb\") as file:\n",
    "    data_snn_bss_7tap = pickle.load(file)\n",
    "\n",
    "print(data_snn_bss_7tap)\n",
    "\n",
    "\n",
    "print(data)\n",
    "\n",
    "params = {'text.usetex' : True,\n",
    "          'font.size' : 8,\n",
    "          }\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "color = [\"#FAC90F\", \"#FA8D0F\", \"#0F69FA\", \"#7A6F45\"]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(3.5, 2.7))\n",
    "axs.set_ylabel(\"BER\")\n",
    "axs.set_xlabel(\"$−10 \\log_{10}(\\sigma^2)$ [dB]\")\n",
    "axs.set_yscale(\"log\")\n",
    "axs.set_ylim(1e-4, 3e-2)\n",
    "axs.set_xlim(14.5, 22.5)\n",
    "axs.set_xticks(data[:, 0])\n",
    "axs.plot(data[:, 0], data[:, 1], lw=1, color=color[0], label=r\"7 tap SNN\")\n",
    "axs.plot(-data_linear_7tap[1:, 0], data_linear_7tap[1:, 1], lw=1, color=color[1], label=r\"7 tap, linear \\cite{arnold2023spiking}\")\n",
    "axs.plot(data_snn_bss_7tap[\"snrs\"], data_snn_bss_7tap[\"bers\"], lw=1, color=color[2], label=r\"7 tap SNN, BSS-2 \\cite{arnold2023spiking}\")\n",
    "axs.scatter(-data_linear_7tap[1:, 0], data_linear_7tap[1:, 1], s=10, color=color[1])\n",
    "axs.scatter(data_snn_bss_7tap[\"snrs\"], data_snn_bss_7tap[\"bers\"], s=10, color=color[2])\n",
    "axs.scatter(data[:, 0], data[:, 1], color=color[0], s=10)\n",
    "axs.hlines(2e-3, 14.5, 22.5, color=\"grey\", ls=\"--\", label=\"KP4 FEC Threshold\")\n",
    "axs.grid(which=\"minor\", lw=0.5)\n",
    "axs.grid(which=\"major\", lw=0.7)\n",
    "axs.yaxis.set_label_coords(-0.1, 0.6)\n",
    "axs.legend(fontsize=9, handlelength=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./snr.pgf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
