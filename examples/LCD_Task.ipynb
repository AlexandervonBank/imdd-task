{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCD-Task: Training a SNN Demapper\n",
    "\n",
    "The SNN demapper used in this example is based on [1].\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "[1] [E. Arnold et al., “Spiking neural network nonlinear\n",
    "demapping on neuromorphic hardware for IM/DD optical communication”](https://ieeexplore.ieee.org/abstract/document/10059327/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import norse.torch as norse\n",
    "\n",
    "from IMDD import LCDDataset, helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive-field Encoding\n",
    "\n",
    "One question when training a spiking demapper is how to best translate a chunk of real-valued data into a spiking representation in an efficient way.\n",
    "The receptive-field encoding translates each samples $y[k]$ in the chunk to a set of $P$ spiking neurons.\n",
    "Each neuron has a `reference_point` assigned and its spike time is determined by the distance of $y[k]$ to the given reference value.\n",
    "This results in a spatio-temporal encoding with $P$ neurons per sample $k$, so $n_\\text{tap}\\cdot P$ input neurons as detailed in [1].\n",
    "\n",
    "We first create implement the encoder as defined in [1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReceptiveFieldEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, scaling: float, offset: float, time_length: float,\n",
    "                 dt: float, references: torch.Tensor, cutoff: float = None):\n",
    "        super().__init__()\n",
    "        self.scaling = scaling\n",
    "        self.offset = offset\n",
    "        self.time_length = time_length\n",
    "        self.dt = dt\n",
    "        self.time_steps = int(time_length // dt) + 1\n",
    "        self.references = references\n",
    "        self.cutoff = cutoff if cutoff is not None else time_length\n",
    "\n",
    "    def forward(self, trace: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \"\"\"\n",
    "        dev = trace.device\n",
    "\n",
    "        # positive spike times\n",
    "        times = self.scaling * torch.abs(\n",
    "            trace.unsqueeze(-1) - self.references.to(dev)).reshape(\n",
    "                trace.shape[0], -1)\n",
    "\n",
    "        times[(times < 0) | (times > self.cutoff)] = self.time_length + self.dt\n",
    "        times += self.offset\n",
    "\n",
    "        bins = (times / self.dt + 1).long()\n",
    "        mask = bins < self.time_steps\n",
    "        mesh = torch.meshgrid([torch.arange(s) for s in times.shape])\n",
    "\n",
    "        indices = torch.stack(\n",
    "            (bins.to(dev)[mask].reshape(-1),\n",
    "             mesh[0].to(dev)[mask].reshape(-1),\n",
    "             *(mesh[i].to(dev)[mask].reshape(-1)\n",
    "               for i in range(1, len(mesh)))))\n",
    "\n",
    "        spikes = torch.sparse_coo_tensor(\n",
    "            indices, torch.ones(indices.shape[1]).to(dev),\n",
    "            (self.time_steps, times.shape[0], *times.shape[1:]), dtype=int)\n",
    "\n",
    "        return spikes.to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the dataset and visualize the input encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LCDDataset()\n",
    "\n",
    "# Generate some data\n",
    "y_chunk, q = dataset[45]\n",
    "\n",
    "# Data\n",
    "print(\"Received symbols (chunked):\\n\", y_chunk, y_chunk.shape)\n",
    "print(\"Corresponding index q:\\n\", q, q.shape)\n",
    "print(\"Corresponding send bits:\\n\", helpers.get_graylabel(2)[q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an instance of the `ReceptiveFieldEncoder`. The encoding is defined by the `references` which we choose to be $10$ values equdistantly distributed in $[0, 7]$. The distance $y_k - \\Chi_i$ is scaled by `scaling`. We neglect spikes which are later than `cutoff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 10\n",
    "references = torch.linspace(0, 7, P)\n",
    "print(\"References: \", references)\n",
    "\n",
    "# Time resolution of encoding and SNN\n",
    "dt = 5e-4\n",
    "time_length = 0.03  # s\n",
    "cutoff = 0.015  # s\n",
    "\n",
    "# The encoer\n",
    "encoder = ReceptiveFieldEncoder(\n",
    "    scaling=0.008,\n",
    "    offset=0.0,\n",
    "    time_length=time_length,\n",
    "    dt=dt,\n",
    "    references=references,\n",
    "    cutoff=cutoff)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode `y_chunk` into a binary spike tensor. The first dimension in the resulting data is the time axis, which is 60, corresponding to 30 ms. This becomes clearer in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = encoder(y_chunk.unsqueeze(0))  # add batch dim\n",
    "print(\"Spikes:\\n\", spikes, spikes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time-dense spikes into event-based spikes\n",
    "events = torch.nonzero(spikes[:, 0])\n",
    "time = np.linspace(0, time_length, int(time_length / dt))\n",
    "\n",
    "fig, axes = plt.subplots(1)\n",
    "axes.set_xlim(0, time_length)\n",
    "axes.set_ylim(0, dataset.simulator.params.n_taps * P)\n",
    "axes.set_xlabel(\"$t$ [ms]\")\n",
    "axes.set_ylabel(\"input neuron $i$\")\n",
    "axes.scatter(events[:, 0] * dt, events[:, 1], s=5, color=\"black\")\n",
    "axes.vlines(cutoff, 0, dataset.simulator.params.n_taps * P, color=\"blue\", ls=\":\")\n",
    "for i in range(7):\n",
    "    axes.hlines(10 * (i + 1), 0, 0.015, color=\"grey\", lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a SNN which we train to solve the demapping task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNDemapper(torch.nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_in: int,\n",
    "                 n_hidden: int,\n",
    "                 n_out: int,\n",
    "                 encoder: torch.nn.Module,\n",
    "                 lif_params: norse.LIFParameters,\n",
    "                 li_params: norse.LIParameters,\n",
    "                 dt: float,\n",
    "                 device):\n",
    "        \"\"\" \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.dt = dt\n",
    "        # Regularization\n",
    "        self.reg_bursts = 0.0005\n",
    "        self.reg_weight_1 = 0.0001\n",
    "        self.reg_weight_2 = 0.0001\n",
    "        self.reg_readout = 0.0\n",
    "        self.target_rate = 0.5\n",
    "\n",
    "        # Encoding symbols to spikes\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # SNN\n",
    "        self.linear_1 = torch.nn.Linear(\n",
    "            n_in, n_hidden, device=device, bias=None)\n",
    "        self.lif = norse.LIFCell(lif_params)\n",
    "        self.linear_2 = torch.nn.Linear(\n",
    "            n_hidden, n_out, device=device, bias=None)\n",
    "        self.li = norse.LICell(li_params)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \"\"\"\n",
    "        self.zi = self.encoder(input).float()\n",
    "\n",
    "        T = self.zi.shape[0]\n",
    "        s_lif, s_li = None, None\n",
    "        zs, ys, s_lifs, s_lis = [], [], [], []\n",
    "        for ts in range(T):\n",
    "            g1 = self.linear_1(self.zi[ts])\n",
    "            z, s_lif = self.lif(g1, s_lif)\n",
    "            g2 = self.linear_2(z)\n",
    "            y, s_li = self.li(g2, s_li)\n",
    "\n",
    "            zs.append(z)\n",
    "            ys.append(y)\n",
    "            s_lifs.append(s_lif)\n",
    "            s_lis.append(s_li)\n",
    "\n",
    "        self.spikes = torch.stack(zs)\n",
    "        self.traces = torch.stack(ys)\n",
    "        self.v_lif = torch.stack([s.v for s in s_lifs])\n",
    "\n",
    "        self.score = torch.amax(self.traces, 0)\n",
    "\n",
    "        return self.score\n",
    "\n",
    "    @property\n",
    "    def rate(self) -> torch.tensor:\n",
    "        return self.zi.sum(0).sum(1), self.spikes.sum(0).sum(1)\n",
    "\n",
    "    def regularize(self) -> torch.Tensor:\n",
    "        \"\"\" Regularization terms for demapper \"\"\"\n",
    "        reg = torch.tensor(0.).to(self.device)\n",
    "        # Regularize linear weights\n",
    "        reg += self.reg_weight_1 * torch.mean(self.linear_1.weight ** 2)\n",
    "        reg += self.reg_weight_2 * torch.mean(self.linear_2.weight ** 2)\n",
    "        # Regularize firing rates\n",
    "        reg += self.reg_bursts * (\n",
    "            (self.target_rate - self.spikes.sum(0)).mean(0) ** 2).mean()\n",
    "        # Regularize readout traces\n",
    "        reg += self.reg_readout * torch.mean(torch.max(self.traces, 0)[0] ** 2)\n",
    "        return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions for training and testing\n",
    "\n",
    "def stats(loss: torch.Tensor, pred: torch.Tensor, data: torch.Tensor):\n",
    "    ber = helpers.bit_error_rate(data, pred, False)\n",
    "    acc = helpers.accuracy(data, pred, False)\n",
    "    count = torch.count_nonzero(torch.argmax(pred, 1) != data)\n",
    "    return ber, acc, count\n",
    "\n",
    "\n",
    "def train(dataloader, optimizer, scheduler, loss_fn, demapper, device):\n",
    "    loss, acc, ber = [], [], []\n",
    "\n",
    "    for i, (data, target) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        pred_b = demapper(data)\n",
    "        loss_b = loss_fn(pred_b, target)\n",
    "        # regularization\n",
    "        loss_b += demapper.regularize()\n",
    "\n",
    "        # Optimize\n",
    "        loss_b.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get stats\n",
    "        ber_b, acc_b, _ = stats(loss_b, pred_b, target)\n",
    "\n",
    "        # Accumualte\n",
    "        loss.append(loss_b.detach())\n",
    "        acc.append(acc_b)\n",
    "        ber.append(ber_b)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return (torch.stack(loss).reshape(-1).mean(),\n",
    "            np.stack(acc).reshape(-1).mean(),\n",
    "            np.stack(ber).reshape(-1).mean())\n",
    "\n",
    "\n",
    "def test(dataloader, demapper, loss_fn, device, min_false_symbols, max_test_epochs):\n",
    "    loss, acc, ber, n_false = [], [], [], 0\n",
    "\n",
    "    for epoch in range(max_test_epochs):\n",
    "        for i, (data, target) in enumerate(dataloader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pred_b = demapper(data)\n",
    "            loss_b = loss_fn(pred_b, target)\n",
    "            loss_b += demapper.regularize()\n",
    "\n",
    "            ber_b, acc_b, count = stats(loss_b, pred_b, target)\n",
    "\n",
    "            loss.append(loss_b.detach())\n",
    "            acc.append(acc_b)\n",
    "            ber.append(ber_b)\n",
    "\n",
    "            n_false += count\n",
    "\n",
    "        if n_false >= min_false_symbols:\n",
    "            break\n",
    "\n",
    "    return (torch.stack(loss).reshape(-1).mean(),\n",
    "            np.stack(acc).reshape(-1).mean(),\n",
    "            np.stack(ber).reshape(-1).mean(), n_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "# Training parameters\n",
    "batch_size_train = 100 \n",
    "batch_size_val = 10000 \n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "max_test_epochs = 100\n",
    "min_false_symbols = 2000\n",
    "min_false_bits = 2000\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Bits\n",
    "gray_bits = torch.tensor(helpers.get_graylabel(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SNN demapper SNR sweep\n",
    "\n",
    "First we train the SNN demapper for a range of noise powers $\\sigma_\\text{n}^2$.\n",
    "We start training with little noise and increase in 1 dB steps.\n",
    "For each noise level we save the model which performs best on the validation data and test it later against unseen data.\n",
    "Note that we continue the training of the SNN across different noise levels.\n",
    "A demapper could also be trained for only one noise level and then be tested across different SNRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixe seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# We create folder to save the models and training data at\n",
    "data_dir = Path(\"../results2/snr\")\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n",
    "model_dir = data_dir / \"models\"\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model\n",
    "lif_demapper = SNNDemapper(\n",
    "    n_in=70,  # n_taps * n_reference_points\n",
    "    n_hidden=40,\n",
    "    n_out=4,  # len(alphabet)\n",
    "    encoder=encoder,\n",
    "    lif_params=norse.LIFParameters(\n",
    "        tau_mem_inv=1/6e-3,\n",
    "        tau_syn_inv=1/6e-3,\n",
    "        v_leak=0.,\n",
    "        v_reset=0.,\n",
    "        v_th=1.),\n",
    "    li_params=norse.LIParameters(\n",
    "        tau_mem_inv=torch.tensor(1/6e-3).to(device),\n",
    "        tau_syn_inv=torch.tensor(1/6e-3).to(device),\n",
    "        v_leak=torch.tensor(0.)),\n",
    "    dt=dt,\n",
    "    device=device)\n",
    "\n",
    "# Dataset\n",
    "dataset = LCDDataset()\n",
    "\n",
    "# Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size_train, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size_val, shuffle=True)\n",
    "\n",
    "# The SNRs we train the demapper for\n",
    "snrs = torch.flip(torch.arange(15., 24., 1.), dims=(0,))\n",
    "snrs[0] = 30. # We train the first demapper with only a little noise\n",
    "\n",
    "# Validation data\n",
    "val_datas = torch.zeros((snrs.shape[0], epochs // 10, 4))\n",
    "\n",
    "# Sweep SNR\n",
    "for i, snr in enumerate(snrs):\n",
    "    print(f\"SNR: {snr.item()}\")\n",
    "    # update SNR in Dataset\n",
    "    dataset.set_noise_power_db(-snr.item())\n",
    "\n",
    "    # New scheduler\n",
    "    optimizer = torch.optim.Adam(lif_demapper.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "    # Train for SNR\n",
    "    val_data = torch.zeros((epochs // 10, 4))\n",
    "    best_val_ber = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc, train_ber = train(\n",
    "            train_loader, optimizer, scheduler, loss_fn, lif_demapper, device)\n",
    "        if (epoch + 1) % 10 == 0 and epoch > 0:\n",
    "            val_loss, val_acc, val_ber, n_false = test(\n",
    "                val_loader, lif_demapper, loss_fn, device, min_false_symbols,\n",
    "                max_test_epochs)\n",
    "            val_datas[i, epoch // 10, 0] = val_loss\n",
    "            val_datas[i, epoch // 10, 1] = val_ber\n",
    "            val_datas[i, epoch // 10, 2] = val_acc\n",
    "            val_datas[i, epoch // 10, 3] = n_false\n",
    "\n",
    "            # Save best Demapper\n",
    "            if val_ber < best_val_ber:\n",
    "                torch.save(\n",
    "                    lif_demapper.state_dict(), model_dir / f\"snr_{int(snr)}.pt\")\n",
    "                best_val_ber = val_ber\n",
    "\n",
    "            print(f\"Epoch={epoch}, val_loss={val_loss:.4f}, \"\n",
    "                  + f\"val_ber={val_ber:.7f}, val_acc={val_acc:.4f}, \"\n",
    "                  + f\"n_false={n_false}\")\n",
    "\n",
    "    # Save data at each SNR in case we interrupt training\n",
    "    np.save(data_dir / \"val_bers.npy\", val_datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we test each SNN for each SNR on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test demapper on independent data for different SNRs\n",
    "snr_data = np.zeros((snrs.shape[0] - 1, 5))\n",
    "\n",
    "for s, snr in enumerate(snrs[1:]):\n",
    "    # Fix seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Dataset and loader\n",
    "    dataset = LCDDataset()\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size_val, shuffle=False)\n",
    "\n",
    "    # Set SNR in dataset\n",
    "    dataset.set_noise_power_db(-snr.item())\n",
    "\n",
    "    # Load best model for current SNR\n",
    "    state_dict = torch.load(model_dir / f\"snr_{int(snr)}.pt\")\n",
    "    lif_demapper.load_state_dict(state_dict)\n",
    "\n",
    "    loss, acc, ber, i_rate, h_rate, n_false = [], [], [], [], [], 0\n",
    "    while True:\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pred_b = lif_demapper(data)\n",
    "\n",
    "            ber_b = helpers.bit_error_rate(target, pred_b, False)\n",
    "            ber.append(ber_b)\n",
    "\n",
    "            i_rate.append(lif_demapper.rate[0].cpu().detach().numpy())\n",
    "            h_rate.append(lif_demapper.rate[1].cpu().detach().numpy())\n",
    "\n",
    "            # Number false bits in current batch\n",
    "            n_false += torch.count_nonzero(\n",
    "                (gray_bits[torch.argmax(pred_b.cpu(), 1)]\n",
    "                 != gray_bits[target.cpu()]).reshape(-1))\n",
    "\n",
    "        if n_false >= min_false_bits:\n",
    "            break\n",
    "\n",
    "    snr_data[s, 0] = snr\n",
    "    snr_data[s, 1] = np.stack(ber).reshape(-1).mean()\n",
    "    snr_data[s, 2] = n_false\n",
    "    snr_data[s, 3] = np.stack(i_rate).reshape(-1).mean()\n",
    "    snr_data[s, 4] = np.stack(h_rate).reshape(-1).mean()\n",
    "\n",
    "    print(f\"Tested Demapper for {snr}. BER = {snr_data[s, 1]}, \" \\\n",
    "          + f\"n_false = {n_false}, rate = {snr_data[s, 4]}\")\n",
    "\n",
    "    np.save(data_dir / \"test_bers.npy\", snr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the BER - SNR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../results/snr/test_bers.npy\")\n",
    "\n",
    "color = [\"#FAC90F\", \"#FA8D0F\", \"#0F69FA\", \"#7A6F45\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(5.5, 2.7))\n",
    "\n",
    "axs[0].set_ylabel(\"BER\")\n",
    "axs[0].set_xlabel(\"$-\\sigma^2_\\mathrm{n}$ [dB]\")\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].set_ylim(1e-4, 3e-2)\n",
    "axs[0].set_xlim(14.5, 22.5)\n",
    "axs[0].grid(which=\"minor\", lw=0.2, ls=\":\")\n",
    "axs[0].grid(which=\"major\", lw=0.7)\n",
    "axs[0].set_xticks(data[:, 0])\n",
    "axs[0].set_yticks([1e-4, 1e-3, 1e-2])\n",
    "axs[0].plot(data[:, 0], data[:, 1], lw=1, color=color[0], label=r\"SNN\")\n",
    "axs[0].scatter(data[:, 0], data[:, 1], color=color[0], s=10)\n",
    "axs[0].hlines(2e-3, 14.5, 22.5, color=\"grey\", ls=\"--\", label=\"KP4 FEC\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_ylabel(\"Spikes\")\n",
    "axs[1].set_xlabel(\"$-\\sigma^2_\\mathrm{n}$ [dB]\")\n",
    "axs[1].set_xticks(data[:, 0])\n",
    "axs[1].set_ylim(0, 95)\n",
    "axs[1].plot(data[:, 0], data[:, 3], lw=1, color=color[0])\n",
    "axs[1].scatter(data[:, 0], data[:, 3], color=color[0], s=10, label=\"Input\")\n",
    "axs[1].plot(data[:, 0], data[:, 4], lw=1, color=color[2])\n",
    "axs[1].scatter(data[:, 0], data[:, 4], color=color[2], s=10, label=\"Hidden\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/snr/ber_snr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep Hidden Size as -20dB\n",
    "\n",
    "Now we repeat the procedure but instead we keep the noise level constant at required value and seep the number of neurons in the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train demapper on independet data\n",
    "data_dir = Path(\"../results2/hidden_size\")\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n",
    "model_dir = data_dir / \"models\"\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "for n_hidden in [5, 10, 15, 20, 30, 40, 60, 100]:\n",
    "    print(f\"n_hidden: {n_hidden}\")\n",
    "    \n",
    "    # reset seed\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    lif_demapper = SNNDemapper(\n",
    "        n_in=70,  # n_taps * n_reference_points\n",
    "        n_hidden=n_hidden,\n",
    "        n_out=4,  # len(alphabet)\n",
    "        encoder=encoder,\n",
    "        lif_params=norse.LIFParameters(\n",
    "            tau_mem_inv=1/6e-3,\n",
    "            tau_syn_inv=1/6e-3,\n",
    "            v_leak=0.,\n",
    "            v_reset=0.,\n",
    "            v_th=1.),\n",
    "        li_params=norse.LIParameters(\n",
    "            tau_mem_inv=torch.tensor(1/6e-3).to(device),\n",
    "            tau_syn_inv=torch.tensor(1/6e-3).to(device),\n",
    "            v_leak=torch.tensor(0.)),\n",
    "        dt=dt,\n",
    "        device=device)\n",
    "\n",
    "    # Dataset\n",
    "    dataset = LCDDataset()\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size_train, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size_val, shuffle=True)\n",
    "\n",
    "    # We pre train at lower noise levels\n",
    "    for i, snr in enumerate(torch.tensor([30, 22, 21, 20])):\n",
    "        print(f\"SNR: {snr.item()}\")\n",
    "        # update SNR in Dataset\n",
    "        dataset.set_noise_power_db(-snr.item())\n",
    "\n",
    "        # New scheduler\n",
    "        optimizer = torch.optim.Adam(lif_demapper.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "        # train for SNR\n",
    "        best_val_ber = np.inf\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc, train_ber = train(\n",
    "                train_loader, optimizer, scheduler, loss_fn, lif_demapper, device)\n",
    "            if (epoch + 1) % 10 == 0 and epoch > 0 and snr == 20.:\n",
    "                val_loss, val_acc, val_ber, n_false = test(\n",
    "                    val_loader, lif_demapper, loss_fn, device, min_false_symbols,\n",
    "                    max_test_epochs)\n",
    "\n",
    "                # Save best Demapper\n",
    "                if val_ber < best_val_ber:\n",
    "                    torch.save(\n",
    "                        lif_demapper.state_dict(),\n",
    "                        model_dir / f\"n_hidden_{n_hidden}.pt\")\n",
    "                    best_val_ber = val_ber\n",
    "\n",
    "                print(f\"Epoch={epoch}, val_loss={val_loss:.4f}, \"\n",
    "                    + f\"val_ber={val_ber:.7f}, val_acc={val_acc:.4f}, \"\n",
    "                    + f\"n_false={n_false}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test demapper on independent data for different hidden sizes\n",
    "datas = np.zeros((8, 5))\n",
    "\n",
    "for s, n_hidden in enumerate([5, 10, 15, 20, 30, 40, 60, 100]):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    lif_demapper = SNNDemapper(\n",
    "        n_in=70,  # n_taps * n_reference_points\n",
    "        n_hidden=n_hidden,\n",
    "        n_out=4,  # len(alphabet)\n",
    "        encoder=encoder,\n",
    "        lif_params=norse.LIFParameters(\n",
    "            tau_mem_inv=1/6e-3,\n",
    "            tau_syn_inv=1/6e-3,\n",
    "            v_leak=0.,\n",
    "            v_reset=0.,\n",
    "            v_th=1.),\n",
    "        li_params=norse.LIParameters(\n",
    "            tau_mem_inv=torch.tensor(1/6e-3).to(device),\n",
    "            tau_syn_inv=torch.tensor(1/6e-3).to(device),\n",
    "            v_leak=torch.tensor(0.)),\n",
    "        dt=dt,\n",
    "        device=device)\n",
    "\n",
    "    # Dataset and loader\n",
    "    dataset = LCDDataset()\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size_val, shuffle=False)\n",
    "\n",
    "    # Load best model for current SNR\n",
    "    state_dict = torch.load(model_dir / f\"n_hidden_{n_hidden}.pt\")\n",
    "    lif_demapper.load_state_dict(state_dict)\n",
    "\n",
    "    loss, acc, ber, n_false, i_rate, h_rate = [], [], [], 0, [], []\n",
    "    while True:\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pred_b = lif_demapper(data)\n",
    "\n",
    "            ber_b = helpers.bit_error_rate(target, pred_b, False)\n",
    "            ber.append(ber_b)\n",
    "            i_rate.append(lif_demapper.rate[0].cpu().detach().numpy())\n",
    "            h_rate.append(lif_demapper.rate[1].cpu().detach().numpy())\n",
    "\n",
    "            n_false += torch.count_nonzero(\n",
    "                (gray_bits[torch.argmax(pred_b.cpu(), 1)]\n",
    "                 != gray_bits[target.cpu()]).reshape(-1))\n",
    "\n",
    "        if n_false >= min_false_bits:\n",
    "            break\n",
    "\n",
    "    datas[s, 0] = n_hidden\n",
    "    datas[s, 1] = np.stack(ber).reshape(-1).mean()\n",
    "    datas[s, 2] = n_false\n",
    "    datas[s, 3] = np.stack(i_rate).reshape(-1).mean()\n",
    "    datas[s, 4] = np.stack(h_rate).reshape(-1).mean()\n",
    "\n",
    "    print(f\"Tested Demapper for {n_hidden}. BER = {datas[s, 1]}, \" \\\n",
    "          + f\"n_false = {n_false}, rate = {datas[s, 4]}\")\n",
    "\n",
    "    np.save(data_dir / \"test_bers.npy\", datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot BER-Hidden Size Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../results/hidden_size/test_bers.npy\")\n",
    "\n",
    "color = [\"#FAC90F\", \"#FA8D0F\", \"#0F69FA\", \"#7A6F45\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(5.5, 2.7))\n",
    "\n",
    "axs[0].set_ylabel(\"BER\")\n",
    "axs[0].set_xlabel(\"Hidden Neurons\")\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[0].grid(which=\"minor\", lw=0.2, ls=\":\")\n",
    "axs[0].grid(which=\"major\", lw=0.7)\n",
    "axs[0].plot(data[:, 0], data[:, 1], lw=1, color=color[0])\n",
    "axs[0].scatter(data[:, 0], data[:, 1], color=color[0], s=10)\n",
    "axs[0].set_ylim(4e-4, 2e-3)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_ylabel(\"Spikes\")\n",
    "axs[1].set_xscale(\"log\")\n",
    "axs[1].set_xlabel(\"Hidden Neurons\")\n",
    "axs[1].set_ylim(0, 95)\n",
    "axs[1].plot(data[:, 0], data[:, 3], lw=1, color=color[0])\n",
    "axs[1].scatter(data[:, 0], data[:, 3], color=color[0], s=10, label=\"Input\")\n",
    "axs[1].plot(data[:, 0], data[:, 4], lw=1, color=color[2])\n",
    "axs[1].scatter(data[:, 0], data[:, 4], color=color[2], s=10, label=\"Hidden\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/hidden_size/ber_hidden_size.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep $n_\\text{taps}$ at -20 dB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train demapper\n",
    "\n",
    "data_dir = Path(\"../results2/n_taps\")\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n",
    "model_dir = data_dir / \"models\"\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "ths = [0.4, 0.7, 0.8, 1, 1, 1, 1, 1, 1]\n",
    "taps = [1, 3, 5, 7, 9, 11, 13, 15, 17]\n",
    "\n",
    "\n",
    "for v_th, n_taps in zip(ths, taps):\n",
    "    print(f\"n_taps: {n_taps}\")\n",
    "\n",
    "    # reset seed\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    lif_demapper = SNNDemapper(\n",
    "        n_in=10*n_taps,  # n_taps * n_reference_points\n",
    "        n_hidden=40,\n",
    "        n_out=4,  # len(alphabet)\n",
    "        encoder=encoder,\n",
    "        lif_params=norse.LIFParameters(\n",
    "            tau_mem_inv=1/6e-3,\n",
    "            tau_syn_inv=1/6e-3,\n",
    "            v_leak=0.,\n",
    "            v_reset=0.,\n",
    "            v_th=v_th),\n",
    "        li_params=norse.LIParameters(\n",
    "            tau_mem_inv=torch.tensor(1/6e-3).to(device),\n",
    "            tau_syn_inv=torch.tensor(1/6e-3).to(device),\n",
    "            v_leak=torch.tensor(0.)),\n",
    "        dt=dt,\n",
    "        device=device)\n",
    "\n",
    "    # Dataset\n",
    "    dataset = LCDDataset(params)\n",
    "    dataset.set_n_taps(n_taps)\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size_train, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size_val, shuffle=True)\n",
    "\n",
    "    for i, snr in enumerate(torch.tensor([30, 22, 21, 20])):\n",
    "        print(f\"SNR: {snr.item()}\")\n",
    "        # update SNR in Dataset\n",
    "        dataset.set_noise_power_gain_db(-snr.item())\n",
    "\n",
    "        # New scheduler\n",
    "        optimizer = torch.optim.Adam(lif_demapper.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "        # train for SNR\n",
    "        best_val_ber = np.inf\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc, train_ber = train(\n",
    "                train_loader, optimizer, scheduler, loss_fn, lif_demapper,\n",
    "                device)\n",
    "            if (epoch + 1) % 10 == 0 and epoch > 0 and snr == 20.:\n",
    "                val_loss, val_acc, val_ber, n_false = test(\n",
    "                    val_loader, lif_demapper, loss_fn, device, min_false_symbols,\n",
    "                    max_test_epochs)\n",
    "\n",
    "                # Save best Demapper\n",
    "                if val_ber < best_val_ber:\n",
    "                    torch.save(\n",
    "                        lif_demapper.state_dict(),\n",
    "                        model_dir / f\"n_taps_{n_taps}.pt\")\n",
    "                    best_val_ber = val_ber\n",
    "\n",
    "                print(f\"Epoch={epoch}, val_loss={val_loss:.4f}, \"\n",
    "                    + f\"val_ber={val_ber:.7f}, val_acc={val_acc:.4f}, \"\n",
    "                    + f\"n_false={n_false}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test demapper on independent data for different n_taps\n",
    "datas = np.zeros((9, 5))\n",
    "\n",
    "\n",
    "for s, (v_th, n_taps) in enumerate(zip(ths, taps)):\n",
    "    # Fix Seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    lif_demapper = SNNDemapper(\n",
    "        n_in=10 * n_taps,  # n_taps * n_reference_points\n",
    "        n_hidden=40,\n",
    "        n_out=4,  # len(alphabet)\n",
    "        encoder=encoder,\n",
    "        lif_params=norse.LIFParameters(\n",
    "            tau_mem_inv=1/6e-3,\n",
    "            tau_syn_inv=1/6e-3,\n",
    "            v_leak=0.,\n",
    "            v_reset=0.,\n",
    "            v_th=v_th),\n",
    "        li_params=norse.LIParameters(\n",
    "            tau_mem_inv=torch.tensor(1/6e-3).to(device),\n",
    "            tau_syn_inv=torch.tensor(1/6e-3).to(device),\n",
    "            v_leak=torch.tensor(0.)),\n",
    "        dt=dt,\n",
    "        device=device)\n",
    "\n",
    "    # Dataset and loader\n",
    "    dataset = PAM4IMDDDataset(params)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size_val, shuffle=False)\n",
    "    dataset.set_noise_power_gain_db(20.)\n",
    "    dataset.set_n_taps(n_taps)\n",
    "\n",
    "    # Load best model for current SNR\n",
    "    state_dict = torch.load(model_dir / f\"/n_taps_{n_taps}.pt\")\n",
    "    lif_demapper.load_state_dict(state_dict)\n",
    "\n",
    "    loss, acc, ber, n_false, i_rate, h_rate = [], [], [], 0, [], []\n",
    "    while True:\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pred_b = lif_demapper(data)\n",
    "\n",
    "            ber_b = helpers.bit_error_rate(target, pred_b, False)\n",
    "            ber.append(ber_b)\n",
    "            i_rate.append(lif_demapper.rate[0].cpu().detach().numpy())\n",
    "            h_rate.append(lif_demapper.rate[1].cpu().detach().numpy())\n",
    "\n",
    "            n_false += torch.count_nonzero(\n",
    "                (gray_bits[torch.argmax(pred_b.cpu(), 1)]\n",
    "                 != gray_bits[target.cpu()]).reshape(-1))\n",
    "\n",
    "        if n_false >= min_false_bits:\n",
    "            break\n",
    "\n",
    "    datas[s, 0] = n_taps\n",
    "    datas[s, 1] = np.stack(ber).reshape(-1).mean()\n",
    "    datas[s, 2] = n_false\n",
    "    datas[s, 3] = np.stack(i_rate).reshape(-1).mean()\n",
    "    datas[s, 4] = np.stack(h_rate).reshape(-1).mean()\n",
    "\n",
    "    print(f\"Tested Demapper for {n_taps}. BER = {datas[s, 1]}, \" \\\n",
    "          + f\"n_false = {n_false}, rate = {datas[s, 4]}\")\n",
    "\n",
    "    np.save(data_dir / \"test_bers.npy\", datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot $n_\\text{taps}$-BER Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../results/n_taps/test_bers.npy\")\n",
    "data[:, 0] = [1, 3, 5, 7, 9, 11, 13, 15, 17]\n",
    "\n",
    "\n",
    "color = [\"#FAC90F\", \"#FA8D0F\", \"#0F69FA\", \"#7A6F45\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(5.5, 2.7))\n",
    "\n",
    "axs[0].set_ylabel(\"BER\")\n",
    "axs[0].set_yticks([1e-3, 5e-4])\n",
    "axs[0].set_xlabel(\"$n_\\mathrm{taps}$\")\n",
    "axs[0].set_ylim(4e-4, 4e-2)\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].grid(which=\"minor\", lw=0.2, ls=\":\")\n",
    "axs[0].grid(which=\"major\", lw=0.7)\n",
    "axs[0].plot(data[:, 0], data[:, 1], lw=1, color=color[0])\n",
    "axs[0].scatter(data[:, 0], data[:, 1], color=color[0], s=10)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_ylabel(\"Spikes\")\n",
    "axs[1].set_xlabel(\"$n_\\mathrm{taps}$\")\n",
    "axs[1].set_ylim(0, 95)\n",
    "axs[1].plot(data[:, 0], data[:, 3], lw=1, color=color[0])\n",
    "axs[1].scatter(data[:, 0], data[:, 3], color=color[0], s=10, label=\"Input\")\n",
    "axs[1].plot(data[:, 0], data[:, 4], lw=1, color=color[2])\n",
    "axs[1].scatter(data[:, 0], data[:, 4], color=color[2], s=10, label=\"Hidden\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/n_taps/n_taps_ber.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
